{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b48327",
   "metadata": {},
   "source": [
    "# Evaluation via LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4ccf0",
   "metadata": {},
   "source": [
    "##### Quellen hierfür: \n",
    "\n",
    "```\n",
    "LangChain. (o. D.-b). ChatPromptTemplate. Abgerufen am 5. Mai 2025, von https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "LangChain. (o. D.-a). ChatOpenAI. Abgerufen am 7. Juni 2025, von https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\n",
    "LangChain. (o. D.-g). StrOutputParser. https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\n",
    "\n",
    "Pandas. (2025, 7. Juli). Pandas Documentation. Abgerufen am 11. Juli 2025, von https://pandas.pydata.org/docs/\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79778453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407ae1e",
   "metadata": {},
   "source": [
    "### LLM Instanziieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ef7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    openai_api_key=os.environ[\"OPEN_AI_API_KEY\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d949d",
   "metadata": {},
   "source": [
    "### Einlesen der CSV mit den Antworten aller Systeme (Suffix_2 stellt den 2. Durchgang dar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33332816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../antworten_der_systeme_manipuliert_beide_durchgänge/graph_rag_vector_rag_hybrid_rag_antworten_manipuliert_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6b850",
   "metadata": {},
   "source": [
    "### Prompt für die Bewertung der Antworten im manipulierten Kontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc52e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../prompts/evaluation_im_manipulierten_kontext.txt\", \"r\", encoding=\"utf-8\") as mani_p:\n",
    "    eval_mani_prompt = mani_p.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a745fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bewertungs_prompt = ChatPromptTemplate.from_template(f\"{eval_mani_prompt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eac3a1",
   "metadata": {},
   "source": [
    "### Prompt für die Bewertung der Antworten im Originalkontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../prompts/evaluation_im_originalkontext.txt\", \"r\", encoding=\"utf-8\") as ori_p:\n",
    "    eval_ori_prompt = ori_p.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad648d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bewertungs_prompt_original = ChatPromptTemplate.from_template(f\"{eval_ori_prompt}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf283a",
   "metadata": {},
   "source": [
    "#### erstellen der Bewertungschain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9175cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "chain_bewertung = (\n",
    "    {\n",
    "        \"artikeltext\": lambda x: x[\"artikeltext\"],\n",
    "        \"frage\": lambda x: x[\"frage\"],\n",
    "        \"fake_news_art\": lambda x: x[\"fake_news_art\"], # auskommentieren wenn original bewertet wird\n",
    "        \"musterloesung\": lambda x: x[\"musterloesung\"],\n",
    "        \"antwort\": lambda x: x[\"antwort\"],\n",
    "    }\n",
    "    | bewertungs_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ae33c",
   "metadata": {},
   "source": [
    "### Bewertungsfunktion für die Antworten der Systeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4751908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def bewertung(df, artikel_pfad, system_spalten:list):\n",
    "    \"\"\" \n",
    "    Funktion für die Bewertung der Antworten der Systeme.\n",
    "    beachte parameter \"system_spalten\" muss mit jeweiligem suffix geändert werden.\n",
    "    beachte außerdem, dass der parameter \"artikel_pfad\" entweder zu dem ordner mit den manipulierten\n",
    "    Artikeln oder mit den Originalartikeln verweist.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Über die jeweiligen Antwortspalten des Dfs iterieren\n",
    "    for system_spalte in system_spalten:\n",
    "        # neue spalte für die Bewertungen des jeweiligen Systems anlegen\n",
    "        bewertung_spalte = f\"{system_spalte}_bewertung\"\n",
    "        # Bewertungen in einer List sammeln\n",
    "        bewertungen = []\n",
    "        # Pfad für den Artikeltext anhand der id finden\n",
    "        for _, row in df.iterrows():\n",
    "            artikel_id = row[\"artikel_id\"]\n",
    "            artikel_datei = f\"artikel_{artikel_id}.txt\"\n",
    "            pfad = os.path.join(artikel_pfad, artikel_datei)\n",
    "\n",
    "            try:\n",
    "                # Artikeltext einlesen\n",
    "                with open(pfad, \"r\", encoding=\"utf-8\") as file:\n",
    "                    artikeltext = file.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Fehler beim Laden von {artikel_datei}: {e}\")\n",
    "                bewertungen.append(\"FEHLER\")\n",
    "                continue\n",
    "            \n",
    "            # Kontext für das LLM extrahieren\n",
    "            eingabe = {\n",
    "                \"artikeltext\": artikeltext,\n",
    "                \"frage\": row[\"frage\"],\n",
    "                \"fake_news_art\": row[\"fake_news_art\"], # bei original auskommentieren\n",
    "                \"musterloesung\": row[\"musterloesung\"],\n",
    "                \"antwort\": row.get(system_spalte, \"\")\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Bewertungschain aufrufen\n",
    "                bewertung = chain_bewertung.invoke(eingabe).strip()\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f\"Fehler bei Bewertung ({system_spalte}) - Artikel {artikel_id}, Frage {row['frage_nr']}: {e}\")\n",
    "                bewertung = \"FEHLER\"\n",
    "                print(eingabe)\n",
    "            # Bewertung der Antwort in die Liste hinzufügen\n",
    "            bewertungen.append(bewertung)\n",
    "        # Neue Spalte mit den Bewertungen der Antworten eines Systems hinzufügen\n",
    "        df[bewertung_spalte] = bewertungen\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbafbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bewertung_llm_judge = bewertung(df,\n",
    "                                   \"../../Datenmanipulation/manipulierte_artikel\",\n",
    "                                    [\"vector_rag_antwort_manipuliert_2\", \"graph_rag_antwort_manipuliert_2\", \"hybrid_rag_antwort_manipuliert_2\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e497bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bewertung_llm_judge.to_csv(\"bewertung_manipulierter_kontext_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb3c89",
   "metadata": {},
   "source": [
    "### Stichprobe für die Reliabilitätsprüfung des LLM-as-a-Judge Ansatzes entnehmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98cb82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stichprobe_fuer_manuelle_bewertung(df, artikel_ordner_pfad, system_spalten:list, n_kombination):\n",
    "\n",
    "    stichprobe_list = []\n",
    "\n",
    "    # über die Antworten der Systeme iterieren\n",
    "    for antwort in system_spalten:\n",
    "            # teilgruppe zu jeder Fake-News-Art\n",
    "        for fake_news_art in df[\"fake_news_art\"].unique():\n",
    "            # erstelle Teilgruppe mit bestimmeter Fake-News-Art\n",
    "            teilgruppe = df[df[\"fake_news_art\"] == fake_news_art]\n",
    "        \n",
    "            #Stichrpobenentnahme\n",
    "            stichprobe = teilgruppe.sample(n=n_kombination, random_state=42).copy()\n",
    "            \n",
    "\n",
    "            stichprobe[\"system\"] = antwort\n",
    "             \n",
    "            stichprobe[\"antwort\"] = stichprobe[antwort]\n",
    "\n",
    "            stichprobe_list.append(stichprobe)\n",
    "\n",
    "    # Stichproben zusammenführen\n",
    "    stichprobe_df = pd.concat(stichprobe_list).reset_index(drop=True)\n",
    "\n",
    "    ######## Artikeltext extahieren ######\n",
    "\n",
    "    artikeltexte = []\n",
    "\n",
    "\n",
    "    # Iteriert über df\n",
    "    for idx, row in stichprobe_df.iterrows():\n",
    "        artikel_id = row[\"artikel_id\"]\n",
    "        artikel_name = f\"artikel_{artikel_id}.txt\"\n",
    "        pfad = os.path.join(artikel_ordner_pfad, artikel_name)\n",
    "\n",
    "        # Artikeltext lesen\n",
    "        with open(pfad, \"r\", encoding=\"utf-8\") as file:\n",
    "            artikeltext = file.read()\n",
    "        \n",
    "        artikeltexte.append(artikeltext)\n",
    "    \n",
    "    # Artikeltext zum DF hinzufügen\n",
    "    stichprobe_df[\"artikeltext\"] = artikeltexte\n",
    "\n",
    "    stichprobe_df[\"manuelle_bewertung\"] = None\n",
    "\n",
    "\n",
    "    return stichprobe_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa07e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_stich = stichprobe_fuer_manuelle_bewertung(df,\n",
    "                                   \"../../Datenmanipulation/manipulierte_artikel\",\n",
    "                                   [\"vector_rag_antwort_manipuliert_2\", \"graph_rag_antwort_manipuliert_2\", \"hybrid_rag_antwort_manipuliert_2\"],\n",
    "                                   7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baca121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stich.to_csv(\"stichprobe_fuer_manuelle_bewertung.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfba140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2088b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
